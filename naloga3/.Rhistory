source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
runif(1,1,2)
source('D:/FRI/3.letnik/IS/assignment1.R')
as.integer(runif(1, 1, 2))
as.integer(runif(1, 1, 2))
as.integer(runif(1, 1, 2))
as.integer(runif(1, 1, 2))
as.integer(runif(1, 1, 3))
as.integer(runif(1, 1, 3))
as.integer(runif(1, 1, 3))
as.integer(runif(1, 1, 3))
as.integer(runif(1, 1, 3))
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
source('D:/FRI/3.letnik/IS/assignment1.R')
text <- "Today's topic is text mining"
text
text <- strsplit(text, split = ' ')
text
text <- text[[1]]
text
source('D:/FRI/3.letnik/IS/assignment2.R')
remove(list = ls())
source('D:/FRI/3.letnik/IS/assignment2.R')
text
names(occurances)
names(hate)
occurances("hate")
occurances
occurances$look
tmp <- c(1:5)
tmp
tmp[1]
tmp[4]
tmp[4] <- NULL
View(dataset)
View(dataset)
?stopwords
?rarewords
??rarewords
corpus
View(corpus)
View(corpus)
corpus[1]
corpus[1]$essay_1.txt
corpus[[1]]$content
test <- "This is a test."
grep("test", test)
test <- "This is a test but not a real test."
grep("test", test)
length(grep("test", test))
test <- "This is a a string."
length(grep("test", test))
source('D:/FRI/3.letnik/IS/assignment2.R')
source('D:/FRI/3.letnik/IS/assignment2.R')
rare_word_dictionary
rare_word_dictionary[¸1]
rare_word_dictionary[1]
rare_word_dictionary[[1]]
View(rare_word_dictionary)
View(rare_word_dictionary)
rare_word_dictionary$V1[1]
rare_word_dictionary$V1
source('D:/FRI/3.letnik/IS/assignment2.R')
doc_rare_word_num(corpus[[1]]$content, rare_word_dictionary)
numeric(0)
integer(numeric(0))
as.integer(numeric(0))
doc_rare_word_num(corpus[[3]]$content, rare_word_dictionary)
doc_rare_word_num(corpus[[99]]$content, rare_word_dictionary)
length(corpus)
source('D:/FRI/3.letnik/IS/assignment2.R')
source('D:/FRI/3.letnik/IS/assignment2.R')
sent_lens
typeof(sent_lens)
typeof(sent_lens[1])
typeof(sent_lens[[1]])
sent_lens
test <- sent_lens[1]
test
remove(test)
rare_words_in_doc <- rare_words_per_doc(corpus, rare_word_dictionary)
remove(list = ls())
source('D:/FRI/3.letnik/IS/naloga3/naloga3.R')
source('D:/FRI/3.letnik/IS/naloga3/naloga3.R')
source('D:/FRI/3.letnik/IS/naloga3/naloga3.R')
source('D:/FRI/3.letnik/IS/naloga3/naloga3.R')
source('D:/FRI/3.letnik/IS/naloga3/naloga3.R')
sent_lens
View(sent_lens)
sent_lens$essay_1.txt
source('D:/FRI/3.letnik/IS/naloga3/naloga3.R')
sent_lents_per_doc
test <- sort(corpus)
source('D:/FRI/3.letnik/IS/naloga3/naloga3.R')
?mixedsort
??mixedsort
install.packages("gtools")
source('D:/FRI/3.letnik/IS/naloga3/naloga3.R')
View(sent_lens)
View(avg_sent_len)
View(avg_sent_len)
View(avg_sent_len)
source('D:/FRI/3.letnik/IS/naloga3/naloga3.R')
mean(sent_lens)
mean(doc_sents)
doc_sents
doc_sents$essay_1.txt
doc_sents$essay_1.txt
doc_sents$essay_1.txt
doc_sents$essay_1.txt
doc_sents$essay_1.txt
doc_sents$essay_1.txt
doc_sents$essay_1.txt
doc_sents$essay_1.txt
doc_sents$essay_1.txt
plot(x = c(1:723), y=sent_lens, type="h", main = "x=document_id y=avg_sen_len")
barplot(x = c(1:723), y=sent_lens, type="h", main = "x=document_id y=avg_sen_len")
doc_words
test <- unlist(sent_lens)
test
test[1]
source('D:/FRI/3.letnik/IS/assignment2.R')
source('D:/FRI/3.letnik/IS/assignment2.R')
source('D:/FRI/3.letnik/IS/assignment2.R')
source('D:/FRI/3.letnik/IS/assignment2.R')
occurances$seat
occurances$time
popular_words
length(popular_words)
source('D:/FRI/3.letnik/IS/assignment2.R')
source('D:/FRI/3.letnik/IS/assignment2.R')
popular_words
popular_words[1]
popular_words[3]
occurances$time
mean(unlist(sent_lens))
median(unlist(sent_lens))
?median
findAssocs(dataset, c("team"), c(0.7))
findAssocs(dataset, c("team"))
findAssocs(dataset, c("team"), c(1))
findAssocs(dataset, c("time"), c(1))
popular_words[1]
findAssocs(dataset, c("ltcap"), c(0.5))
findAssocs(dataset, c("ltcap"), c(0.9))
popular_words[5]
findAssocs(dataset, c("min"), c(0.9))
findAssocs(dataset, c("min"), c(0.1))
findAssocs(dataset, c("min"), c(0.5))
test <- findAssocs(dataset, c("min"), c(0.5))
test
test[1]
test$min
test[[¸1]]
test[[1]]
test <- unlink(test)
test
source('D:/FRI/3.letnik/IS/assignment2.R')
doc words
doc_words
723 * 0,3
723 * 0.3
723 * 0.3
Vocabulary[1]
Vocabulary[1][1]
Vocabulary$V1
Vocabulary$V1[1]
Vocabulary$V1[1:3]
vocabulary_test <- c(Vocabulary$V1[1:217])
vocabulary_learn <- c(Vocabulary$V1[218:723])
structure_learn <- c(Structure$V1[218:723])
structure_test <- c(Structure$V1[1:217])
typeof(unlist(sent_lens))
sent_lens_vec <- unlist(sent_lens)
sent_lens_vec
mean(sent_lens_vec)
sent_lens_vec[1]
typeof(sent_lens_vec[1])
doc_sents_vec <- unlist(doc_sents)
doc_sents_vec
doc_words_vec <- unlist(doc_words)
sent_lens_test <- sent_lens_vec[1:217]
doc_sents_test <- doc_sents_vec[1:217]
doc_words_test <- doc_words_vec[1:217]
sent_lens_learn <- sent_lens_vec[218:723]
doc_sents_learn <- doc_sents_vec[218:723]
doc_words_learn <- doc_words_vec[218:723]
data_learn <- cbind(c("AvgSenLen", sent_lens_learn), c("SenCount", doc_sents_learn), c("WordCount",doc_words_learn))
data_learn
data_learn <- cbind(c("AvgSenLen", sent_lens_learn), c("SenCount", doc_sents_learn), c("WordCount",doc_words_learn), c("Class", vocabulary_learn))
data_learn
data_learn[1]
data_learn[2]
data_learn[[1]]
data_learn[1]
data_learn[1:]
data_learn[1:5]
data_learn[2]
data_learn[3]
data_learn[500]
data_learn[700]
data_learn[1,]
data_learn[2,]
data_learn[,1]
data_learn[,2]
data_learn[,3]
data_learn[1,]
data_learn[2,]
data_learn
rare_word_dictionary <- read.table("my_rare_dict.txt")
doc_rare_word_num <- function(doc, dict) {
count <- 0
for (word in dict$V1) {
count <- count + grep(doc, word)
}
return(count)
}
rare_words_per_doc <- function(corpus, dict) {
doc_list <- c()
for (i in c(1:length(corpus))) {
doc_list <- c(doc_list, doc_rare_word_num(corpus[[1]]$content, dict))
}
return(doc_list)
}
rare_words_in_doc <- rare_words_per_doc(corpus, rare_word_dictionary)
data_learn
CA <- function(observed, predicted)
{
t <- table(observed, predicted)
sum(diag(t)) / sum(t)
}
pred_vocabulary <- vocabulary_test
data_learn_frame <- data.frame(data_learn)
data_learn_frame
data_learn <- cbind(c(sent_lens_learn), c(doc_sents_learn), c(doc_words_learn), c(vocabulary_learn))
data_learn_frame <- data.frame(data_learn)
data_learn_frame
dataset$v
dataset[3,]$v
science_dict <- read.table("science.txt")
science_dict
science_dict <- tm_map(science_dict, stemDocument)
typeof(science_dict)
typeof(corpus)
science_dict <- tm_map(data.frame(science_dict), stemDocument)
science_dict <- tm_map(science_dict, stemDocument)
science_dict <- tm_map(list(science_dict), stemDocument)
dataset[3,]$v
which(dataset[5,]$v < 0.002, arr.ind = T)
length(which(dataset[5,]$v < 0.002, arr.ind = T))
length(which(dataset[5,]$v < 0.005, arr.ind = T))
length(which(dataset[5,]$v < 0.002, arr.ind = T))
length(which(dataset[5,]$v < 0.000, arr.ind = T))
doc_rare_word <- function(dataset) {
rare_list <- c()
for (i in c(1:723)) {
rare_list <- c(rare_list, length(which(dataset[5,]$v < 0.002, arr.ind = T)))
}
return(rare_list)
}
rare_list <- doc_rare_word(dataset)
rare_list
doc_rare_word <- function(dataset) {
rare_list <- c()
for (i in c(1:723)) {
rare_list <- c(rare_list, length(which(dataset[i,]$v < 0.002, arr.ind = T)))
}
return(rare_list)
}
rare_list <- doc_rare_word(dataset)
rare_list
data_learn <- cbind(c(sent_lens_learn), c(doc_sents_learn),
c(doc_words_learn), c(rare_list), c(vocabulary_learn))
data_learn <- cbind(c(sent_lens_learn), c(doc_sents_learn), c(doc_words_learn), c(rare_list), c(vocabulary_learn))
data_learn <- cbind(c(sent_lens_learn), c(doc_sents_learn), c(doc_words_learn), c(rare_list), c(vocabulary_learn))
data_learn
science_dict <- read.table("science.txt")
science_dict
science_dict <- read.table("science.txt")
science_dict <- read.table("science.txt")
science_dict
science_dict <- read.table("science.txt")
science_dict
str <- "la la ladida la"
grep(str, la)
grep(str, "la")
length(grep(str, "la"))
length(grepl(str, "la"))
grepl(str, "la")
grepl(str, "ladida")
grepl(str, "l")
grepl(str, c("la"))
grep("la", str)
science_dict
for (word in science_dict) {
}
for (word in science_dict) {
print(word)
}
grep("la", str)
source('D:/FRI/3.letnik/IS/assignment2.R')
doc_mydict_num <- function(doc, dict) {
count <- 0
for (word in dict) {
count <- count + grep(word, doc)
}
return(count)
}
doc_mydict_num(corpus[[1]]$content, science_dict)
doc_mydict_num <- function(doc, dict) {
count <- 0
for (word in dict) {
if(grep(word, doc) > 0) {
count <- count +  1
}
}
return(count)
}
doc_mydict_num(corpus[[1]]$content, science_dict)
source('D:/FRI/3.letnik/IS/assignment2.R')
install.packages(c("rpart", "rpart.plot", "CORElearn", "e1071", "randomForest"))
library(e1071)
sm <- svm(Class ~ ., data = data_learn_frame)
data_learn
data_learn_frame <- data.frame(data_learn)
data_learn_frame
sm <- svm(X5 ~ ., data = data_learn_frame)
predicted <- predict(sm, vocabulary_test, type="class")
sm
rare_list_test <- rare_list[1:217]
rare_list_learn <- rare_list[218:723]
data_learn <- cbind(c(sent_lens_learn), c(doc_sents_learn), c(doc_words_learn), c(rare_list_learn), c(vocabulary_learn))
data_learn_frame <- data.frame(data_learn)
data_learn_frame
sm <- svm(X5 ~ ., data = data_learn_frame)
sm
data_test <- cbind(c(sent_lens_test), c(doc_sents_test), c(doc_words_test), c(rare_list_test))
data_test_frame <- data.frame(data_test)
predicted <- predict(sm, data_learn_frame, type="class")
predicted
CA(vocabulary_test, predicted)
predicted <- round(predicted)
predicted
predicted <- predict(sm, data_test_frame, type="class")
CA(vocabulary_test, predicted)
predicted <- round(predicted)
CA(vocabulary_test, predicted)
predicted <- predict(sm, data_test_frame, type="class")
predicted <- round(predicted)
CA(vocabulary_test, predicted)
predicted <- predict(sm, data_test_frame, type="class")
predicted <- round(predicted)
CA(vocabulary_test, predicted)
popular_words
library(CORElearn)
source('D:/FRI/3.letnik/IS/assignment2.R')
cm.nb <- CoreModel(X5 ~ ., data = data_learn_frame, model="bayes")
predicted <- predict(cm.nb, data_test_frame, type="class")
predicted <- round(predicted)
CA(predicted, vocabulary_test)
cm.nb <- CoreModel(X5 ~ ., data = data_learn_frame, model="bayes")
predicted <- predict(cm.nb, data_test_frame, type="class")
predicted <- round(predicted)
CA(predicted, vocabulary_test)
cm.rf <- CoreModel(X5 ~ ., data = data_learn_frame, model="rf")
predicted <- predict(cm.rf, data_test_frame, type="class")
predicted <- round(predicted)
CA(predicted, vocabulary_test)
cm.rf <- CoreModel(X5 ~ ., data = data_learn_frame, model="rf")
predicted <- predict(cm.rf, data_test_frame, type="class")
predicted <- round(predicted)
CA(predicted, vocabulary_test)
library(rpart)
dt <- rpart(X5 ~ ., data = data_learn_frame)
predicted <- predict(dt, data_test_frame, type="class")
predicted <- predict(dt, data_test_frame, type="class")
predicted <- predict(dt, data_test_frame, type="class")
dt <- rpart(X5 ~ ., data = data_learn_frame)
predicted <- predict(dt, data_test_frame, type="class")
predicted <- predict(dt, data_test, type="class")
ksm <- ksvm(X5 ~ ., data = data_learn_frame,kernel = "rbfdot")
predicted <- predict(ksm, data_test_frame, type="class")
predicted <- predict(ksm, data_test_frame)
predict()
predicted
predicted <- round(predicted)
CA(predicted, vocabulary_test)
install.packages("openNLPmodels.en", repos="http://datacube.wu.ac.at/", type="source")
text <- "The allies after nassau in december 1960, the U.S. first proposed to help nato develop its own nuclear strike force. But europe made no attempt to devise a plan."
sent_ann <- Maxent_Sent_Token_Annotator()
a <- annotate(text, sent_ann)
a
View(a)
a[1]
a[[1]]
a[[2]]
as.String(text)[a]
word_ann <- Maxent_Word_Token_Annotator()
a2 <- annotate(text, word_ann, a)
a2
post_ann <- Maxent_POS_Tag_Annotator()
a3 <- annotate(text, post_ann, a2)
a3
as.String(text)[3]
as.String(text)[a3]
text <- corpus[[1]]$content
text
corpus <- tm_map(corpus, removePunctuation)
text <- corpus[[1]]$content
source('D:/FRI/3.letnik/IS/assignment2.R')
source('D:/FRI/3.letnik/IS/naloga3/naloga3.R')
